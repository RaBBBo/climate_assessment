{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e750bd",
   "metadata": {},
   "source": [
    "Created on \n",
    "\n",
    "@author: Elham Goumehei based on \"20221229_VI.py\"\n",
    "\n",
    "Crop and animal water footprint is calculated using global average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b6f88",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37dea9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyxlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import jenkspy\n",
    "import numpy as np\n",
    "# import sys\n",
    "import csv\n",
    "import openpyxl\n",
    "import networkx as nx \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "input_path = \"C:/Users/BloemendaalB1/GIT/test_repo/proxies\"\n",
    "# output_path = \"C:/Users/GoumeheiE1/OneDrive - Raboweb/Documents/Impact_level/Data/VI/results_test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85cb08d",
   "metadata": {},
   "source": [
    "## 2 Read databases for VIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f19774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.846772667108111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GoumeheiE1\\AppData\\Local\\Temp\\ipykernel_56784\\2982118965.py:47: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  euro_area_rate = float(e_rates[e_rates['Country'] == 'Euro area']['e_rate'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EUROSTAT_ENV_WAT_ABS = pd.read_excel((input_path + \"water_abstraction.xlsx\"),\n",
    "                            sheet_name = None) \n",
    "db_map_EUROSTAT = pd.read_excel((input_path + \"Data_map.xlsx\"),\n",
    "                         sheet_name = 'Eurostat')\n",
    "# print(f\"db_map_EUROSTAT {db_map_EUROSTAT.head()}\")\n",
    "# print(f\"EUROSTAT_ENV_WAT_ABS {EUROSTAT_ENV_WAT_ABS}\")\n",
    "EUKLEMS_INTANPROD_naccounts = pd.read_csv((input_path + \"national accounts.csv\")\n",
    "                                          ,quotechar = '\"'\n",
    "                                          ,quoting=1\n",
    "                                          ,doublequote = True) \n",
    "# print(f\"EUKLEMS_INTANPROD_naccounts {EUKLEMS_INTANPROD_naccounts.head()}\")\n",
    "EUKLEMS_INTANPROD_caccounts = pd.read_csv((input_path + \"capital accounts.csv\")\n",
    "                                          ,quotechar = '\"'\n",
    "                                          ,quoting=1\n",
    "                                          ,doublequote = True) \n",
    "# print(f\"EUKLEMS_INTANPROD_caccounts {EUKLEMS_INTANPROD_caccounts.head()}\")\n",
    "\n",
    "# Read in database sector mapping to heatmap sectors\n",
    "db_map_EUKLEMS = pd.read_excel((input_path + \"Data_map.xlsx\"),\n",
    "                         sheet_name = 'EUKLEMS')\n",
    "# print(f\"db_map_EUKLEMS {db_map_EUKLEMS.head()}\")\n",
    "\n",
    "IEA = pd.read_excel((input_path + \"IEA EEI database_Highlights.xlsb\"),\n",
    "                    sheet_name=['Services - Energy', 'Industry - Energy', 'Transport - Energy']) \n",
    "db_map_IEA = pd.read_excel((input_path + \"Data_map.xlsx\"),\n",
    "                         sheet_name = 'IEA')\n",
    "\n",
    "#Read in currency exchange rates\n",
    "e_rates = pd.read_csv((input_path + \"API_PA.NUS.FCRF_DS2_en_csv_v2_4772354.csv\")\n",
    "                      ,quotechar = '\"'\n",
    "                      ,quoting=1\n",
    "                      ,doublequote = True\n",
    "                      ,skiprows=[0,1,2,3]\n",
    "                      ,on_bad_lines='skip')\n",
    "\n",
    "\n",
    "# reformat e_rates db\n",
    "e_rates = e_rates.loc[:,['Country Name','2018']]\n",
    "e_rates = e_rates.rename(columns={e_rates.columns[0]:'Country',\n",
    "                                  e_rates.columns[1]:'e_rate'})\n",
    "\n",
    "# Assign e_rates to euro area countries in 2018\n",
    "euro_area = ['Austria', 'Belgium','Cyprus','Estonia','Finland'\n",
    "             ,'France','Germany','Greece','Ireland','Italy','Latvia'\n",
    "             ,'Lithuania','Luxembourg','Malta','Netherlands','Portugal'\n",
    "             ,'Slovakia','Slovenia','Spain']\n",
    "euro_area_rate = float(e_rates[e_rates['Country'] == 'Euro area']['e_rate'])\n",
    "print(euro_area_rate)\n",
    "e_rates.loc[e_rates['Country'].isin(euro_area),'e_rate'] = euro_area_rate\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff28026",
   "metadata": {},
   "source": [
    "## 3 Create NR VI db for country x sector from EUROSTAT_ENV_WAT_ABS db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c053438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NR_data_all \n",
      "          Country   Value  Sector\n",
      "0        Belgium   54.46   1.1.1\n",
      "1       Bulgaria  723.59   1.1.1\n",
      "2        Czechia    47.7   1.1.1\n",
      "3        Denmark  659.34   1.1.1\n",
      "4        Estonia     5.2   1.1.1\n",
      "..           ...     ...     ...\n",
      "677   Luxembourg    0.21  11.1.1\n",
      "678        Malta     0.5  11.1.1\n",
      "679  Netherlands       0  11.1.1\n",
      "680       Poland  220.89  11.1.1\n",
      "681       Serbia   28.21  11.1.1\n",
      "\n",
      "[682 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read and reformat database structure\n",
    "db_structure = EUROSTAT_ENV_WAT_ABS['Summary'].drop(index=range(13)) # Drop rows without data\n",
    "db_structure = db_structure.drop(columns=db_structure.columns[0], axis=1) # Drop first column without data\n",
    "db_structure.columns = db_structure.iloc[0] # Set first row as column name\n",
    "db_structure = db_structure.tail(-1) # Remove first row\n",
    "# print(f\"db_structure \\n {db_structure}\")\n",
    "\n",
    "# Create empty db to store outpu\n",
    "NR_data_all = pd.DataFrame(columns = ['Country','Value','Sector'])\n",
    "\n",
    "\n",
    "# Iterate for each sector and find NR data\n",
    "for index,row in db_map_EUROSTAT.iterrows():\n",
    "    \n",
    "    # Find sheet of sector based on db map\n",
    "    sheet = db_structure[\n",
    "        (db_structure ['Water process'] == row['Water process'])\n",
    "        &(db_structure ['Water sources'] == row['Water sources'])\n",
    "        &(db_structure ['Unit of measure'] == row['Unit of measure'])\n",
    "        ]['Contents'].item()\n",
    "\n",
    "    # Keep data for 2018 and remove all others\n",
    "    r,c = np.where(EUROSTAT_ENV_WAT_ABS[sheet]=='2018')\n",
    "    NR_data = EUROSTAT_ENV_WAT_ABS[sheet].iloc[int(r)+2:,[0,int(c)]]\n",
    "    # Drop nan values\n",
    "    NR_data= NR_data[\n",
    "        (NR_data.iloc[:,1].notna())\n",
    "        & (NR_data.iloc[:,1] != ':')\n",
    "        ]\n",
    "#     print(f\"NR_data \\n {NR_data}\")\n",
    "    # Reform NR_Data\n",
    "    NR_data = NR_data.rename(columns={NR_data.columns[0]: \"Country\",\n",
    "                                   NR_data.columns[1]: \"Value\"}\n",
    "                          )   \n",
    "#     print(f\"NR_data \\n {NR_data}\")\n",
    "    NR_data['Sector'] = row['Heatmap_sector_level_3_cd'] \n",
    "\n",
    "#     NR_data_all = NR_data_all.append(NR_data, ignore_index = True)\n",
    "    NR_data_all = pd.concat([NR_data_all,NR_data], ignore_index = True)\n",
    "\n",
    "# NR_data_all[\"Value\"] = NR_data_all[\"Value\"].astype(str)\n",
    "\n",
    "\n",
    "print(f\"NR_data_all \\n {NR_data_all}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd9a11",
   "metadata": {},
   "source": [
    "## 4 Create ES VI db for country x sector from IEA db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0324c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES_data_all \n",
      "              Country    Value  Sector\n",
      "0            Albania     4.95   1.1.1\n",
      "1          Argentina   145.55   1.1.1\n",
      "2            Armenia     2.95   1.1.1\n",
      "3          Australia   112.04   1.1.1\n",
      "4            Austria    22.44   1.1.1\n",
      "...              ...      ...     ...\n",
      "1843     Switzerland   133.85  11.1.1\n",
      "1844          Turkey   538.62  11.1.1\n",
      "1845  United Kingdom   773.19  11.1.1\n",
      "1846   United States  9067.51  11.1.1\n",
      "1847         Uruguay    13.36  11.1.1\n",
      "\n",
      "[1848 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#read database\n",
    "IEA = pd.read_excel((input_path + \"IEA EEI database_Highlights.xlsb\"),\n",
    "                    sheet_name=['Services - Energy', 'Industry - Energy', 'Transport - Energy']) \n",
    "db_map_IEA = pd.read_excel((input_path + \"Data_map.xlsx\"),\n",
    "                         sheet_name = 'IEA')\n",
    "\n",
    "\n",
    "ES_data_all = pd.DataFrame(columns = ['Country','Value','Sector'])\n",
    "\n",
    "# Iterate for each sector and find NR data\n",
    "for index,row in db_map_IEA.iterrows():\n",
    "#     print(row['Heatmap_sector_level_3_cd'])\n",
    "    # if row['Heatmap_sector_level_3_cd'] == '10.1.2': #'1.1.1': #\n",
    "    #     break\n",
    "\n",
    "    #Some sectors have to no data, so assume skip these\n",
    "    try:\n",
    "        # Find sheet of sector based on db map\n",
    "        sheet = row['Sheet']\n",
    "        # Find sector mapping \n",
    "        # Some heatmap sectors map to multiple db sectors\n",
    "        smap = row['Subsector'].split(\", \")\n",
    "        \n",
    "        ES_data = IEA[sheet]\n",
    "        ES_data.columns = ES_data.iloc[0] # Set first row as column names \n",
    "        ES_data = ES_data.tail(-1) # Then delete first row\n",
    "        ES_data = ES_data[ES_data.iloc[:,1].isin(smap)]\n",
    "        \n",
    "        # Keep data for 2018 and remove all others\n",
    "        ES_data = ES_data.loc[:,['Country',2018]]\n",
    "        # Drop no data values\n",
    "        ES_data= ES_data[(ES_data[2018] != '..')\n",
    "            ]\n",
    "        \n",
    "        # Sum data for the same country\n",
    "        # Some heatmap sectors map to multiple db sectors\n",
    "        ES_data = ES_data.groupby('Country').sum()\n",
    "          \n",
    "        # Reformat ER_Data\n",
    "        ES_data = ES_data.rename(columns={ES_data.columns[0]: \"Value\"})\n",
    "        ES_data['Country'] = ES_data.index\n",
    "        ES_data['Sector'] = row['Heatmap_sector_level_3_cd'] \n",
    "    \n",
    "#         ES_data_all = ES_data_all.append(ES_data, ignore_index = True)\n",
    "        ES_data_all = pd.concat([ES_data_all,ES_data], ignore_index = True)\n",
    "    except:\n",
    "        continue\n",
    "print(f\"ES_data_all \\n {ES_data_all}\")\n",
    "# print(f\"smap \\n {smap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f61c12",
   "metadata": {},
   "source": [
    "## 5 Create RM,T,OPEX,CAPEX,BA,WF VI db for country x sector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e2355b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### from EUKLEMS_INTANPROD dbs and convert all national currency to USD\n",
    "# print(f\"smap \\n {smap}\")\n",
    "def EUKLEMS(variables,db_input,smap, convert = False, e_rates = e_rates):\n",
    "     \n",
    "    data = db_input[db_input['nace_r2_code'].isin(smap)]\n",
    "#     print(f\"data00 \\n \\n {data}\")\n",
    "    # Select relavant variable only\n",
    "    data = data.loc[:,['nace_r2_code','geo_name', 'year']+variables]\n",
    "#     print(f\"data11 \\n \\n {data}\")\n",
    "    \n",
    "    # Keep data for 2018 and remove all others\n",
    "    data = data[data['year']==2018]\n",
    "    # Drop no data values\n",
    "    data = data[data[variables].notna().all(axis=1)]\n",
    "    #Drop irrelevant columns\n",
    "    data = data.drop(columns=['nace_r2_code','year'])\n",
    "#     print(f\"data22 \\n \\n {data}\")\n",
    "     \n",
    "###################################################################    \n",
    "    # Sum data for the same country\n",
    "    # Some heatmap sectors map to multiple db sectors\n",
    "#     data = data.groupby('geo_name').sum()\n",
    "    data = data.groupby(['geo_name']).sum()\n",
    "#     print(f\"dataAB \\n \\n {data}\")\n",
    "    # Sum across variables\n",
    "    # Some proxy datasets map to multiple variables (e.g. CAPEX)\n",
    "    data = data.sum(axis = 1)\n",
    "    #print(f\"dataBB \\n \\n {data}\")\n",
    "    \n",
    "#################################################################    \n",
    "    \n",
    "    # Reformat Data\n",
    "    data = pd.DataFrame(data)\n",
    "    data = data.rename(columns={data.columns[0]: \"Value\"})\n",
    "#     print(f\"dataAA renamed column \\n \\n {data}\")\n",
    "    data['Country'] = data.index\n",
    "    data['Sector'] = row['Heatmap_sector_level_3_cd'] \n",
    "#     print(f\"dataCC{data.head()}\")\n",
    "#     print(f\"e_rates{e_rates}\")\n",
    "    \n",
    "    if convert == True:\n",
    "        # Assign e_rate for countries\n",
    "        data = data.rename(columns={'Value':'Value_NAC'})\n",
    "        data = pd.merge(data,e_rates)\n",
    "#         print(f\"data22{data}\")\n",
    "        # Convert NAC to USD\n",
    "        data['Value'] = data['Value_NAC']/data['e_rate']\n",
    "        data = data.drop(columns=['Value_NAC','e_rate'])\n",
    "#         print(f\"data converted to USD \\n {data}\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b640f",
   "metadata": {},
   "source": [
    "## 6 Create output db for all VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d84bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RM_data_all = pd.DataFrame(columns = ['Country','Value','Sector'])\n",
    "T_data_all = pd.DataFrame(columns = ['Country','Value','Sector'])\n",
    "OPEX_data_all = pd.DataFrame(columns = ['Country','Value','Sector'])\n",
    "CAPEX_data_all = pd.DataFrame(columns = ['Country','Value','Sector'])\n",
    "BA_data_all = pd.DataFrame(columns = ['Country','Value','Sector'])\n",
    "WF_data_all = pd.DataFrame(columns = ['Country','Value','Sector'])\n",
    "\n",
    "\n",
    "# Iterate for each sector and find NR data\n",
    "for index,row in db_map_EUKLEMS.iterrows():\n",
    "#     print(row['Heatmap_sector_level_3_cd'])\n",
    "    \n",
    "    #Some sectors have no data, so skip these\n",
    "    try:\n",
    "        # Find sector mapping \n",
    "        # Some heatmap sectors map to multiple db sectors\n",
    "        smap = row['nace_r2_code'].split(\";\")\n",
    "        \n",
    "        RM_data = EUKLEMS(variables = ['II_CP'],\n",
    "                          db_input = EUKLEMS_INTANPROD_naccounts,\n",
    "                          smap = smap,\n",
    "                          convert = True)\n",
    "       ## print(f\"RM_data {RM_data}\")\n",
    "#         RM_data_all = RM_data_all.append(RM_data, ignore_index = True)\n",
    "        RM_data_all = pd.concat([RM_data_all, RM_data], ignore_index = True)\n",
    "        \n",
    "        \n",
    "        T_data = EUKLEMS(variables = ['I_TraEq'],\n",
    "                          db_input = EUKLEMS_INTANPROD_caccounts,\n",
    "                          smap = smap,\n",
    "                          convert = True)\n",
    "#         T_data_all = T_data_all.append(T_data, ignore_index = True)\n",
    "        T_data_all = pd.concat([T_data_all, T_data], ignore_index = True)\n",
    "        \n",
    "        OPEX_data = EUKLEMS(variables = ['I_GFCF'],\n",
    "                          db_input = EUKLEMS_INTANPROD_caccounts,\n",
    "                          smap = smap,\n",
    "                          convert = True)\n",
    "#         OPEX_data_all = OPEX_data_all.append(OPEX_data, ignore_index = True)\n",
    "        OPEX_data_all = pd.concat([OPEX_data_all, OPEX_data], ignore_index = True)\n",
    "        \n",
    "        CAPEX_data = EUKLEMS(variables = ['K_IT','K_CT','K_Soft_DB','K_TraEq'\n",
    "                                          ,'K_OMach','K_OCon','K_Rstruc'\n",
    "                                          ,'K_RD', 'K_OIPP']\n",
    "                             ,db_input = EUKLEMS_INTANPROD_caccounts\n",
    "                             ,smap = smap,\n",
    "                             convert = True)\n",
    "#         CAPEX_data_all = CAPEX_data_all.append(CAPEX_data, ignore_index = True)\n",
    "        CAPEX_data_all = pd.concat([CAPEX_data_all, CAPEX_data], ignore_index = True)\n",
    "        \n",
    "        BA_data = EUKLEMS(variables = ['K_Cult'],\n",
    "                          db_input = EUKLEMS_INTANPROD_caccounts,\n",
    "                          smap = smap,\n",
    "                          convert = True)\n",
    "#         BA_data_all = BA_data_all.append(BA_data, ignore_index = True)\n",
    "        BA_data_all = pd.concat([BA_data_all, BA_data], ignore_index = True)\n",
    "        \n",
    "        WF_data = EUKLEMS(variables = ['H_EMP'],\n",
    "                          db_input = EUKLEMS_INTANPROD_naccounts,\n",
    "                          smap = smap)\n",
    "#         WF_data_all = WF_data_all.append(WF_data, ignore_index = True)\n",
    "        WF_data_all = pd.concat([WF_data_all, WF_data], ignore_index = True)\n",
    "    except:\n",
    "        continue\n",
    "# print(f\"WF_data_all {WF_data_all}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2830db4b",
   "metadata": {},
   "source": [
    "# 7 GVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b7a12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GVA_data_all             Country          Value  Sector\n",
      "0           Austria    5162.542640   1.1.1\n",
      "1           Belgium    3073.670303   1.1.1\n",
      "2          Bulgaria    2231.929392   1.1.1\n",
      "3           Croatia    1839.457486   1.1.1\n",
      "4            Cyprus     432.465541   1.1.1\n",
      "..              ...            ...     ...\n",
      "990        Slovenia    3008.717805  10.1.2\n",
      "991           Spain   58119.495245  10.1.2\n",
      "992          Sweden   12448.751426  10.1.2\n",
      "993  United Kingdom   93810.595316  10.1.2\n",
      "994   United States  596600.000000  10.1.2\n",
      "\n",
      "[995 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#%% Get GVA for country x sector from EUKLEMS_INTANPROD dbs and convert to USD from NAC\n",
    "\n",
    "# Create empty output database\n",
    "GVA_data_all = pd.DataFrame(columns = ['Country','Value','Sector'])\n",
    "\n",
    "# Iterate for each sector and find NR data\n",
    "for index,row in db_map_EUKLEMS.iterrows():\n",
    "#     print(row['Heatmap_sector_level_3_cd'])\n",
    "    # if row['Heatmap_sector_level_3_cd'] == '3.1.1': #'10.1.2': #\n",
    "    #     break\n",
    "    #Some sectors have no data, so assume skip these\n",
    "    try:\n",
    "        # Find sector mapping \n",
    "        # Some heatmap sectors map to multiple db sectors\n",
    "        smap = row['nace_r2_code'].split(\";\")\n",
    "        GVA_data = EUKLEMS(variables = ['VA_CP'],\n",
    "                          db_input = EUKLEMS_INTANPROD_naccounts,\n",
    "                          smap = smap,\n",
    "                          convert=True)\n",
    "        # Drop those with 0 value added\n",
    "        GVA_data = GVA_data[GVA_data['Value'] != 0]\n",
    "        #print(f\"GVA_data {GVA_data}\")\n",
    "        # Append sector data to output db\n",
    "#         GVA_data_all = GVA_data_all.append(GVA_data, ignore_index = True) \n",
    "        GVA_data_all = pd.concat([GVA_data_all, GVA_data], ignore_index = True)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "print(f\"GVA_data_all {GVA_data_all}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1ad6a",
   "metadata": {},
   "source": [
    "# 8 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df8fb647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES_data_all              Country    Value  Sector\n",
      "0            Albania     4.95   1.1.1\n",
      "1          Argentina   145.55   1.1.1\n",
      "2            Armenia     2.95   1.1.1\n",
      "3          Australia   112.04   1.1.1\n",
      "4            Austria    22.44   1.1.1\n",
      "...              ...      ...     ...\n",
      "1843     Switzerland   133.85  11.1.1\n",
      "1844          Turkey   538.62  11.1.1\n",
      "1845  United Kingdom   773.19  11.1.1\n",
      "1846   United States  9067.51  11.1.1\n",
      "1847         Uruguay    13.36  11.1.1\n",
      "\n",
      "[1848 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#%% Normalise countryxsector VI by countryxsector GVA (IntOutput_Normalisation_VI.csv)\n",
    "\n",
    "def GVA_norm(VI,\n",
    "             VI_data,\n",
    "             db_map_data,\n",
    "             db_group,\n",
    "             GVA_data = GVA_data_all,\n",
    "             db_map_GVA = db_map_EUKLEMS):\n",
    "#     print(f\"VI_data {VI_data}\")\n",
    "    # Find group overlap between GVA and VI dbs\n",
    "    # Sector groups for GVA and VI must match before normalisation\n",
    "    VI_data = pd.merge(VI_data,db_map_data, left_on='Sector',right_on = 'Heatmap_sector_level_3_cd')\n",
    "    db_map_data = db_map_data.set_index(db_map_data['Heatmap_sector_level_3_cd'])\n",
    "    data_groups = db_map_data.groupby(by = db_group).groups\n",
    "#     print(f\"data_groups {data_groups}\")\n",
    "    \n",
    "    GVA_data = pd.merge(GVA_data,db_map_GVA, left_on='Sector',right_on = 'Heatmap_sector_level_3_cd')\n",
    "    db_map_GVA = db_map_GVA.set_index(db_map_data['Heatmap_sector_level_3_cd']) \n",
    "    GVA_groups = db_map_GVA.groupby(by = 'nace_r2_code').groups\n",
    "    #print(f\"GVA_data {GVA_data}\")\n",
    "    \n",
    "    VI_data_groups = VI_data.groupby(['Country', db_group],as_index=False).first()\n",
    "    GVA_data_groups = GVA_data.groupby(['Country', 'nace_r2_code'],as_index=False).first()\n",
    "    \n",
    "    merge_group = data_groups | GVA_groups\n",
    "    group_list = list(merge_group.values())\n",
    "    \n",
    "    G=nx.Graph()\n",
    "    for l in group_list:\n",
    "        nx.add_path(G,l)\n",
    "    conn_group = list(nx.connected_components(G))\n",
    "    \n",
    "    \n",
    "    # Create empy normalise dataframe\n",
    "    data_norm = pd.DataFrame(columns=['Country','Sector','Value_'+VI\n",
    "                                     ,'Value_GVA','Value_norm_'+VI])\n",
    "    \n",
    "    # Create group map\n",
    "    for group in conn_group:\n",
    "        rel_groups_GVA = []\n",
    "        rel_groups_VI = []\n",
    "        \n",
    "        rel_groups_GVA += {i for i in GVA_groups if any(group.intersection(GVA_groups[i]))}  \n",
    "        rel_groups_VI += {i for i in data_groups if any(group.intersection(data_groups[i]))}  \n",
    "        \n",
    "        # Get unique values only\n",
    "        rel_groups_GVA = list(set(rel_groups_GVA))\n",
    "        rel_groups_VI = list(set(rel_groups_VI))\n",
    "        \n",
    "        select_GVA = GVA_data_groups[GVA_data_groups['nace_r2_code'].isin(rel_groups_GVA)]\n",
    "        #print(f\"select_GVA {select_GVA}\")\n",
    "        sum_GVA = select_GVA.groupby(by='Country',as_index=False)['Value'].sum()\n",
    "        #print(f\"sum_GVA {sum_GVA}\")\n",
    "        \n",
    "        select_VI = VI_data_groups[VI_data_groups[db_group].isin(rel_groups_VI)]\n",
    "        sum_VI= select_VI.groupby(by='Country',as_index=False)['Value'].sum()\n",
    "        \n",
    "        data_merge= pd.merge(sum_VI, sum_GVA, \n",
    "                            on='Country',\n",
    "                            suffixes = ['_'+VI,'_GVA'])\n",
    "        \n",
    "        data_merge['Value_norm_'+VI] = (data_merge['Value_'+VI]\n",
    "                                   /data_merge['Value_GVA'])\n",
    "        for sector in group:\n",
    "            data_merge['Sector'] = sector\n",
    "            #print(f\"data_norm00 {data_norm}\")\n",
    "            #print(f\"data_merge {data_merge}\")\n",
    "            data_norm = pd.concat([data_norm,data_merge], ignore_index = True)\n",
    "#             data_norm = data_norm.append(data_merge, ignore_index = True)\n",
    "#     print(f\"data_norm {data_norm}\")\n",
    "    return data_norm\n",
    "        \n",
    "NR_data_all_norm = GVA_norm(VI = 'NR'\n",
    "                            ,db_map_data = db_map_EUROSTAT\n",
    "                            ,db_group = 'Water process'\n",
    "                            ,VI_data = NR_data_all)\n",
    "# print(f\"NR_data_all {NR_data_all}\")\n",
    "\n",
    "ES_data_all_norm = GVA_norm(VI = 'ES'\n",
    "                            ,db_map_data = db_map_IEA\n",
    "                            ,db_group = 'Subsector'\n",
    "                            ,VI_data = ES_data_all)\n",
    "print(f\"ES_data_all {ES_data_all}\")\n",
    "\n",
    "RM_data_all_norm = GVA_norm(VI = 'RM'\n",
    "                            ,db_map_data = db_map_EUKLEMS\n",
    "                            ,db_group = 'nace_r2_code'\n",
    "                            ,VI_data = RM_data_all)\n",
    "\n",
    "T_data_all_norm = GVA_norm(VI = 'T'\n",
    "                            ,db_map_data = db_map_EUKLEMS\n",
    "                            ,db_group = 'nace_r2_code'\n",
    "                            ,VI_data = T_data_all)\n",
    "\n",
    "OPEX_data_all_norm = GVA_norm(VI = 'OPEX'\n",
    "                            ,db_map_data = db_map_EUKLEMS\n",
    "                            ,db_group = 'nace_r2_code'\n",
    "                            ,VI_data = OPEX_data_all)\n",
    "\n",
    "CAPEX_data_all_norm = GVA_norm(VI = 'CAPEX'\n",
    "                            ,db_map_data = db_map_EUKLEMS\n",
    "                            ,db_group = 'nace_r2_code'\n",
    "                            ,VI_data = CAPEX_data_all)\n",
    "\n",
    "BA_data_all_norm = GVA_norm(VI = 'BA'\n",
    "                            ,db_map_data = db_map_EUKLEMS\n",
    "                            ,db_group = 'nace_r2_code'\n",
    "                            ,VI_data = BA_data_all)\n",
    "\n",
    "WF_data_all_norm = GVA_norm(VI = 'WF'\n",
    "                            ,db_map_data = db_map_EUKLEMS\n",
    "                            ,db_group = 'nace_r2_code'\n",
    "                            ,VI_data = WF_data_all)\n",
    "# print(f\"NR_data_all_norm {NR_data_all_norm.head()}\")\n",
    "# print(f\"ES_data_all_norm {ES_data_all_norm.head()}\")\n",
    "NR_data_all_norm.to_csv((output_path +'IntOutput_Normalisation_NR.csv'), sep=';',decimal=',',float_format = '%.2f')   \n",
    "ES_data_all_norm.to_csv((output_path +'IntOutput_Normalisation_ES.csv'), sep=';',decimal=',',float_format = '%.2f')   \n",
    "RM_data_all_norm.to_csv((output_path +'IntOutput_Normalisation_RM.csv'), sep=';',decimal=',',float_format = '%.2f') \n",
    "T_data_all_norm.to_csv((output_path +'IntOutput_Normalisation_T.csv'), sep=';',decimal=',',float_format = '%.2f') \n",
    "OPEX_data_all_norm.to_csv((output_path +'IntOutput_Normalisation_OPEX.csv'), sep=';',decimal=',',float_format = '%.2f') \n",
    "CAPEX_data_all_norm.to_csv((output_path +'IntOutput_Normalisation_CAPEX.csv'), sep=';',decimal=',',float_format = '%.2f') \n",
    "BA_data_all_norm.to_csv((output_path +'IntOutput_Normalisation_BA.csv'), sep=';',decimal=',',float_format = '%.2f') \n",
    "WF_data_all_norm.to_csv((output_path +'IntOutput_Normalisation_WF.csv'), sep=';',decimal=',',float_format = '%.2f') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db328d58",
   "metadata": {},
   "source": [
    "# 9 Get global averages value per sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "407d3e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%% Get global averages value per sector\n",
    "# Sectors w/out private individuals as this was separately calculated.\n",
    "sectors = pd.read_excel((input_path +'Data_map.xlsx'),\n",
    "                         sheet_name = 'EUKLEMS',\n",
    "                         usecols=['Heatmap_sector_level_3_cd'])\n",
    "Global_VInorm = pd.DataFrame(index=sectors['Heatmap_sector_level_3_cd'],\n",
    "                         columns=['NR','ES','RM','T','OPEX',\n",
    "                                  'CAPEX','BA','WF'])\n",
    "    \n",
    "Global_VInorm['NR'] = NR_data_all_norm.groupby(by='Sector')['Value_norm_NR'].mean()\n",
    "Global_VInorm['ES'] = ES_data_all_norm.groupby(by='Sector')['Value_norm_ES'].mean()\n",
    "Global_VInorm['RM'] = RM_data_all_norm.groupby(by='Sector')['Value_norm_RM'].mean()\n",
    "Global_VInorm['T'] = T_data_all_norm.groupby(by='Sector')['Value_norm_T'].mean()\n",
    "Global_VInorm['OPEX'] = OPEX_data_all_norm.groupby(by='Sector')['Value_norm_OPEX'].mean()\n",
    "Global_VInorm['CAPEX'] = CAPEX_data_all_norm.groupby(by='Sector')['Value_norm_CAPEX'].mean()\n",
    "Global_VInorm['BA'] = BA_data_all_norm.groupby(by='Sector')['Value_norm_BA'].mean()\n",
    "Global_VInorm['WF'] = WF_data_all_norm.groupby(by='Sector')['Value_norm_WF'].mean()\n",
    "\n",
    "\n",
    "Global_VI= pd.DataFrame(index=sectors['Heatmap_sector_level_3_cd'],\n",
    "                         columns=['NR','ES','RM','T','OPEX',\n",
    "                                  'CAPEX','BA','WF'])\n",
    "\n",
    "Global_VI['NR'] = NR_data_all_norm.groupby(by='Sector')['Value_NR'].mean()\n",
    "Global_VI['ES'] = ES_data_all_norm.groupby(by='Sector')['Value_ES'].mean()\n",
    "Global_VI['RM'] = RM_data_all_norm.groupby(by='Sector')['Value_RM'].mean()\n",
    "Global_VI['T'] = T_data_all_norm.groupby(by='Sector')['Value_T'].mean()\n",
    "Global_VI['OPEX'] = OPEX_data_all_norm.groupby(by='Sector')['Value_OPEX'].mean()\n",
    "Global_VI['CAPEX'] = CAPEX_data_all_norm.groupby(by='Sector')['Value_CAPEX'].mean()\n",
    "Global_VI['BA'] = BA_data_all_norm.groupby(by='Sector')['Value_BA'].mean()\n",
    "Global_VI['WF'] = WF_data_all_norm.groupby(by='Sector')['Value_WF'].mean()\n",
    "\n",
    "Global_GVA= pd.DataFrame(index=sectors['Heatmap_sector_level_3_cd'],\n",
    "                         columns=['GVA'])\n",
    "\n",
    "Global_GVA['GVA'] = ES_data_all_norm.groupby(by='Sector')['Value_GVA'].mean()\n",
    "\n",
    "Global_VInorm.to_csv((output_path +'Global_norm_VI.csv'), sep=';',decimal=',')   \n",
    "Global_VI.to_csv((output_path +'Global_VI.csv'), sep=';',decimal=',') \n",
    "Global_GVA.to_csv((output_path +'Global_GVA.csv'), sep=';',decimal=',')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893f9dd",
   "metadata": {},
   "source": [
    "# 10 Disaggregate Agricultural Sector using water footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "156b082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:79: UserWarning: Unknown extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "#%% Disaggregate Agricultural Sector using water footprint\n",
    "\n",
    "WF_crop = pd.read_excel((input_path +'Report47-Appendix-II.xlsx'),\n",
    "                         sheet_name='App-II-WF_perTon',\n",
    "                         skiprows=[0,1,2,3],\n",
    "                         usecols=[3,8,9])\n",
    "WF_crop= WF_crop.rename(columns={WF_crop.columns[0] : 'Product description (HS)'\n",
    "                                      ,WF_crop.columns[1] : 'WF'\n",
    "                                      ,WF_crop.columns[2] : 'Global average'})\n",
    "WF_crop_edit = WF_crop.copy(deep=True)\n",
    "for index,row in WF_crop.iterrows():  \n",
    "    if pd.isna(row['Product description (HS)']):\n",
    "        continue\n",
    "    else:\n",
    "        WF_crop_edit.loc[index:index+2, 'Product description (HS)'] = row['Product description (HS)']\n",
    "WF_crop_edit = WF_crop_edit[WF_crop_edit['WF']=='Blue']\n",
    "\n",
    "WF_animal = pd.read_excel((input_path +'Report48-Appendix-V.xlsx'),\n",
    "                         sheet_name='App-V_WF_HS_SITC',\n",
    "                         skiprows=[0,1],\n",
    "                         usecols=[2,8,12])\n",
    "WF_animal = WF_animal.rename(columns={WF_animal.columns[0] : 'Product description (HS)'\n",
    "                                      ,WF_animal.columns[1] : 'WF'\n",
    "                                      ,WF_animal.columns[2] : 'Global average'})\n",
    "WF_animal_edit = WF_animal.copy(deep=True)\n",
    "for index,row in WF_animal.iterrows():  \n",
    "    if pd.isna(row['Product description (HS)']):\n",
    "        continue\n",
    "    else:\n",
    "        WF_animal_edit.loc[index:index+2, 'Product description (HS)'] = row['Product description (HS)']\n",
    "WF_animal_edit = WF_animal_edit[WF_animal_edit['WF']=='Blue']\n",
    "\n",
    "db_map_WF = pd.read_excel((input_path +'Data_map.xlsx'),\n",
    "                         sheet_name = 'WF')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dad01b",
   "metadata": {},
   "source": [
    "# 11 Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb914729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GoumeheiE1\\AppData\\Local\\Temp\\ipykernel_56784\\2302855620.py:29: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  WF_crop_average = float(WF_data_ag.loc[crop_sectors].mean())\n",
      "C:\\Users\\GoumeheiE1\\AppData\\Local\\Temp\\ipykernel_56784\\2302855620.py:34: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  WF_animal_average = float(WF_data_ag.loc[animal_sectors].mean())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create empty dataframe\n",
    "WF_data_ag = pd.DataFrame(columns = ['Water use'],\n",
    "                          index=db_map_WF['Heatmap_sector_level_3_cd'])\n",
    "\n",
    "# Read data from database\n",
    "for index,row in db_map_WF.iterrows():    \n",
    "#     print(row['Heatmap_sector_level_3_cd'])\n",
    "    # if row['Heatmap_sector_level_3_cd'] == '1.1.2':\n",
    "    #     break\n",
    "    #Some sectors have no data, so assume skip these\n",
    "    try:\n",
    "        sector = row['Heatmap_sector_level_3_cd']\n",
    "        smap = row['Product description (HS)'].split(\";\")\n",
    "        \n",
    "        if row['Type'] == 'Crop':\n",
    "            WF_data_ag.loc[sector,'Water use'] = float(WF_crop_edit[WF_crop_edit[\n",
    "                'Product description (HS)'].isin(smap)]['Global average']\n",
    "                .mean())\n",
    "        else:\n",
    "            WF_data_ag.loc[sector,'Water use'] = float(WF_animal_edit[WF_animal_edit[\n",
    "                'Product description (HS)'].isin(smap)]['Global average']\n",
    "                .mean())\n",
    "    except:\n",
    "        continue\n",
    "# Fill sectors with no data.\n",
    "# Use the average of crop sectors for floriculture and logging. \n",
    "crop_sectors = ['1.1.1','1.1.2','1.1.3','1.2.1','1.2.2','1.2.3']\n",
    "nan_crop_sectors =['1.3.1', '1.4.1']\n",
    "WF_crop_average = float(WF_data_ag.loc[crop_sectors].mean())\n",
    "WF_data_ag.loc[nan_crop_sectors,'Water use'] = WF_crop_average\n",
    "# Use the average of protein sectors for Remaining Animal Protein \n",
    "animal_sectors = ['1.5.1','1.6.1','1.8.1','1.9.1','1.10.1']\n",
    "nan_animal_sectors =['1.10.2']\n",
    "WF_animal_average = float(WF_data_ag.loc[animal_sectors].mean())\n",
    "WF_data_ag.loc[nan_animal_sectors,'Water use'] = WF_animal_average\n",
    "# Use 0 for aquaculture and wildcatch.\n",
    "aq_sectors =['1.7.1', '1.7.2']\n",
    "WF_data_ag.loc[aq_sectors,'Water use'] = 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339ac7e6",
   "metadata": {},
   "source": [
    "# 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "875db5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate water use ratio\n",
    "WF_av = WF_data_ag['Water use'].mean()\n",
    "# Fill no data sectors with average to maintain the NR/GVA average over the\n",
    "# Sector\n",
    "WF_data_ag['Water use ratio'] = WF_data_ag['Water use']/WF_av\n",
    "new_NR = (Global_VInorm.loc[WF_data_ag.index,'NR']*WF_data_ag['Water use ratio']).dropna()\n",
    "\n",
    "Final_VI_00 = Global_VInorm.copy(deep = True)\n",
    "Final_VI_00.loc[new_NR.index,'NR'] = new_NR\n",
    "\n",
    "\n",
    "WF_data_ag.to_csv((output_path +'WaterFootprint_Ag.csv'), sep=';',decimal=',')  \n",
    "Final_VI_00.to_csv((output_path +'VI.csv'), sep=';',decimal=',')  \n",
    "\n",
    "#%% For sectors with primarily NL exposure (listed in NL_sectors) use NL data only \n",
    "\n",
    "# Sectors with primarily NL exposure (listed in NL_sectors)\n",
    "NL_exposure = ['5.1.1','9.1.1','7.1.1','7.1.2','7.1.3','7.2.1','8.1.1',\n",
    "               '8.1.2','10.1.1','10.1.2']\n",
    "# Create db to change data\n",
    "Final_VI_01 = Final_VI_00.copy(deep = True)\n",
    "\n",
    "def NL_data_check(VI,db,sector,output_db = Final_VI_01):\n",
    "    NL_data= db[(db['Country']=='Netherlands')\n",
    "                      &(db['Sector']==sector)][\n",
    "                          'Value_norm_'+VI]                  \n",
    "    if len(NL_data) > 0:\n",
    "        NL_data = NL_data.item()\n",
    "        output_db.loc[sector,VI]=NL_data\n",
    "        \n",
    "    return output_db\n",
    "\n",
    "for sector in NL_exposure:\n",
    "#     print(sector)\n",
    "    # Check if there is data for NL. If there is replace data.\n",
    "    Final_VI_01 = NL_data_check('NR',NR_data_all_norm,sector)\n",
    "    Final_VI_01 = NL_data_check('ES',ES_data_all_norm,sector)\n",
    "    Final_VI_01 = NL_data_check('RM',RM_data_all_norm,sector)\n",
    "    Final_VI_01 = NL_data_check('T',T_data_all_norm,sector)\n",
    "    Final_VI_01 = NL_data_check('OPEX',OPEX_data_all_norm,sector)\n",
    "    Final_VI_01 = NL_data_check('CAPEX',CAPEX_data_all_norm,sector)\n",
    "    Final_VI_01 = NL_data_check('BA',BA_data_all_norm,sector)\n",
    "    Final_VI_01 = NL_data_check('WF',WF_data_all_norm,sector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479c6ac",
   "metadata": {},
   "source": [
    "# 13 Add mortgage sector VI's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "259137c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GoumeheiE1\\AppData\\Local\\Temp\\ipykernel_56784\\2162028624.py:6: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  NL_H_spending = float(H_spending[H_spending['LOCATION']=='NLD']['Value'])\n",
      "C:\\Users\\GoumeheiE1\\AppData\\Local\\Temp\\ipykernel_56784\\2162028624.py:14: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  NL_H_price = float(H_price[H_price['LOCATION']=='NLD']['Value'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%% Add mortgage sector VI's\n",
    "# Household spending to income ratio as normalised OPEX\n",
    "H_spending = pd.read_csv((input_path +'DP_LIVE_18102022164331234.csv'),\n",
    "                         sep = ';')\n",
    "# Only use NL data since this is the primary country where the bank has exposure\n",
    "NL_H_spending = float(H_spending[H_spending['LOCATION']=='NLD']['Value'])\n",
    "# Convert % to decimal notation\n",
    "NL_H_spending = NL_H_spending/100\n",
    "\n",
    "# Household price to income ratio as normalised CAPEX\n",
    "H_price = pd.read_csv((input_path +'DP_LIVE_18102022160852406.csv'),\n",
    "                         sep = ',')\n",
    "# Only use NL data since this is the primary country where the bank has exposure\n",
    "NL_H_price = float(H_price[H_price['LOCATION']=='NLD']['Value'])\n",
    "# Convert % to decimal notation\n",
    "NL_H_price = NL_H_price/100\n",
    "\n",
    "# Append to VI\n",
    "Mortgage_VI = pd.DataFrame({\n",
    "    \"OPEX\": NL_H_spending,\n",
    "    \"CAPEX\": NL_H_price\n",
    "}, index=[\"12.1.1\"])\n",
    "\n",
    "\n",
    "# Final_VI_00 = Final_VI_00.append(Mortgage_VI)\n",
    "Final_VI_00 = pd.concat([Final_VI_00,Mortgage_VI])\n",
    "# Final_VI_01 = Final_VI_01.append(Mortgage_VI)\n",
    "Final_VI_01 = pd.concat([Final_VI_01,Mortgage_VI])\n",
    "Final_VI_01.to_csv((output_path +'VI_wNL.csv'), sep=';',decimal=',')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4815eb2",
   "metadata": {},
   "source": [
    "# 14 Min/max normalisation VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c2a8f7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Final_VI_00_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     max_value \u001b[38;5;241m=\u001b[39m Final_VI_00[VI]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m      5\u001b[0m     min_value \u001b[38;5;241m=\u001b[39m Final_VI_00[VI]\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m----> 6\u001b[0m     Final_VI_00_norm[VI] \u001b[38;5;241m=\u001b[39m (Final_VI_00[VI] \u001b[38;5;241m-\u001b[39m min_value) \u001b[38;5;241m/\u001b[39m (max_value \u001b[38;5;241m-\u001b[39m min_value)\n\u001b[0;32m      9\u001b[0m Final_VI_00_norm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNR\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Final_VI_00_norm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNR\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     10\u001b[0m Final_VI_00_norm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Final_VI_00_norm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mES\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Final_VI_00_norm' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#%% Min/max normalisation VI    \n",
    "# Final_VI_00_norm = Final_VI_00.copy(deep=True)\n",
    "for VI in Final_VI_00.columns:\n",
    "    max_value = Final_VI_00[VI].max()\n",
    "    min_value = Final_VI_00[VI].min()\n",
    "    Final_VI_00_norm[VI] = (Final_VI_00[VI] - min_value) / (max_value - min_value)\n",
    "\n",
    "\n",
    "Final_VI_00_norm[\"NR\"] = Final_VI_00_norm[\"NR\"].astype(float).fillna(0).round(2)\n",
    "Final_VI_00_norm[\"ES\"] = Final_VI_00_norm[\"ES\"].astype(float).fillna(0).round(2)\n",
    "Final_VI_00_norm[\"RM\"] = Final_VI_00_norm[\"RM\"].astype(float).fillna(0).round(2)\n",
    "Final_VI_00_norm[\"T\"] = Final_VI_00_norm[\"T\"].astype(float).fillna(0).round(2)\n",
    "Final_VI_00_norm[\"OPEX\"] = Final_VI_00_norm[\"OPEX\"].astype(float).fillna(0).round(2)\n",
    "Final_VI_00_norm[\"CAPEX\"] = Final_VI_00_norm[\"CAPEX\"].astype(float).fillna(0).round(2)\n",
    "Final_VI_00_norm[\"BA\"] = Final_VI_00_norm[\"BA\"].astype(float).fillna(0).round(2)\n",
    "Final_VI_00_norm[\"WF\"] = Final_VI_00_norm[\"WF\"].astype(float).fillna(0).round(2)\n",
    "\n",
    "\n",
    "# Final_VI_00_norm.dtypes\n",
    "Final_VI_00_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #%% Min/max normalisation VI    \n",
    "# Final_VI_00_norm = Final_VI_00.copy(deep=True)\n",
    "# for VI in Final_VI_00.columns:\n",
    "#     max_value = Final_VI_00[VI].max()\n",
    "#     min_value = Final_VI_00[VI].min()\n",
    "#     Final_VI_00_norm[VI] = (Final_VI_00[VI] - min_value) / (max_value - min_value)\n",
    "\n",
    "\n",
    "Final_VI_00_norm.to_csv((output_path +'VI_norm.csv'), sep=';',decimal=',')   \n",
    "\n",
    "Final_VI_01_norm = Final_VI_01.copy(deep=True)\n",
    "for VI in Final_VI_01.columns:\n",
    "    max_value = Final_VI_01[VI].max()\n",
    "    min_value = Final_VI_01[VI].min()\n",
    "    Final_VI_01_norm[VI] = (Final_VI_01[VI] - min_value) / (max_value - min_value)\n",
    "    \n",
    "Final_VI_01_norm.to_csv((output_path +'VI_norm_wNL.csv'), sep=';',decimal=',')   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55177fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
